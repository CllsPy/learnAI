{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e134551e",
   "metadata": {
    "papermill": {
     "duration": 0.006534,
     "end_time": "2024-05-02T10:56:35.925396",
     "exception": false,
     "start_time": "2024-05-02T10:56:35.918862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook Objective\n",
    "---\n",
    "\n",
    "This notebook documents a class taught to the former AI President of Tesla, Andrej Karpathy, on building a GPT (Generative Pre-trained Transformer). The code presented is not my own, but rather an attempt to understand the technology in a more technical manner. Regarding the dataset, originally Shakespeare's text (Tiny Shakespeare) was used, but I chose to use \"The Little Prince\" to evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a12e5",
   "metadata": {
    "papermill": {
     "duration": 0.005741,
     "end_time": "2024-05-02T10:56:35.937258",
     "exception": false,
     "start_time": "2024-05-02T10:56:35.931517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What I Learned\n",
    "---\n",
    "During the class, I delved into the fascinating world of GPT technology and discovered that its essence is rooted in the concept of transformers, originating from the paper \"Attention is All You Need.\" To build something as impressive as generating new reference texts, I learned that there are some key steps to follow:\n",
    "\n",
    "- Choosing the Data: Carefully selecting the data is crucial. They will serve as the foundation upon which the GPT will learn and generate new content.\n",
    "\n",
    "- Tokenizing the Data: This process involves breaking down the data into smaller parts, such as words or subwords, so that the neural network can understand and process them efficiently.\n",
    "\n",
    "- Organizing the Data: The data needs to be organized in a way that the neural network can predict the next elements based on the patterns it identifies. For example, if the word \"Hello\" was encoded as 11 22 33 44 55, the code needs to be able to predict what will come after increasing combinations of these elements, such as 11 22, 11 22 33, and so on.\n",
    "\n",
    "This progressive learning process allows the GPT to absorb information from the training data and subsequently generate new texts based on this solid foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62a2e9",
   "metadata": {
    "papermill": {
     "duration": 0.005608,
     "end_time": "2024-05-02T10:56:35.948847",
     "exception": false,
     "start_time": "2024-05-02T10:56:35.943239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba1fa57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:35.961849Z",
     "iopub.status.busy": "2024-05-02T10:56:35.961586Z",
     "iopub.status.idle": "2024-05-02T10:56:35.977184Z",
     "shell.execute_reply": "2024-05-02T10:56:35.976375Z"
    },
    "id": "ns86z13G2K2v",
    "papermill": {
     "duration": 0.024287,
     "end_time": "2024-05-02T10:56:35.979069",
     "exception": false,
     "start_time": "2024-05-02T10:56:35.954782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('/kaggle/input/pequeno-p-text/pequenop.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842f5e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:35.992260Z",
     "iopub.status.busy": "2024-05-02T10:56:35.991715Z",
     "iopub.status.idle": "2024-05-02T10:56:35.996113Z",
     "shell.execute_reply": "2024-05-02T10:56:35.995304Z"
    },
    "id": "zhue9UZI2N4N",
    "papermill": {
     "duration": 0.012953,
     "end_time": "2024-05-02T10:56:35.998075",
     "exception": false,
     "start_time": "2024-05-02T10:56:35.985122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  77454\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7327ea9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:36.011022Z",
     "iopub.status.busy": "2024-05-02T10:56:36.010775Z",
     "iopub.status.idle": "2024-05-02T10:56:36.014711Z",
     "shell.execute_reply": "2024-05-02T10:56:36.013968Z"
    },
    "id": "dwaS5TKt2XVG",
    "papermill": {
     "duration": 0.012803,
     "end_time": "2024-05-02T10:56:36.016868",
     "exception": false,
     "start_time": "2024-05-02T10:56:36.004065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      "\n",
      "Certa vez, quando tinha seis anos, vi num livro sobre a Floresta Virgem, \"Historias \n",
      "Vividas\", uma imponente gravura. Representava ela uma jiboia que engolia uma fera. Eis \n",
      "a copia do desenho. \n",
      "\n",
      "\n",
      "\n",
      "Dizia o livro: \"As jiboias engolem, sem mastigar, a presa inteira. Em seguida, nao \n",
      "podem mover-se e dormem os seis meses da digestao.\" \n",
      "\n",
      "Refleti muito entao sobre as aventuras da selva, e fiz, com lapis de cor, o meu \n",
      "primeiro desenho. Meu desenho numero 1 era assim: \n",
      "\n",
      "\n",
      "\n",
      "Mostrei minha obra-prima as pessoas grandes e perguntei se o meu desenho lhes \n",
      "fazia medo. \n",
      "\n",
      "Responderam-me: \"Por que e que um chapeu faria medo?\" \n",
      "\n",
      "Meu desenho nao representava um chapeu. Representava uma jiboia digerindo um \n",
      "elefante. Desenhei entao o interior da jiboia, a fim de que as pessoas grandes pudessem \n",
      "compreender. Elas tern sempre necessidade de explicates . Meu desenho numero 2 era \n",
      "assim: \n",
      "\n",
      "\n",
      "\n",
      "As pessoas grandes aconselharam-me deixar de lado os desenhos de jiboias abertas \n",
      "ou fechadas, e dedicar-me de pref\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcdc1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:36.030121Z",
     "iopub.status.busy": "2024-05-02T10:56:36.029850Z",
     "iopub.status.idle": "2024-05-02T10:56:36.035696Z",
     "shell.execute_reply": "2024-05-02T10:56:36.034809Z"
    },
    "id": "bx5b_wCZ6wTh",
    "papermill": {
     "duration": 0.01451,
     "end_time": "2024-05-02T10:56:36.037511",
     "exception": false,
     "start_time": "2024-05-02T10:56:36.023001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'()*,-./012356789:;=>?ABCDEFGHIJLMNOPQRSTUVXZ\\^abcdefghijlmnopqrstuvxyz¥—“”\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ba891b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:36.050966Z",
     "iopub.status.busy": "2024-05-02T10:56:36.050722Z",
     "iopub.status.idle": "2024-05-02T10:56:36.056768Z",
     "shell.execute_reply": "2024-05-02T10:56:36.055858Z"
    },
    "id": "w9FEr-KS9cV5",
    "papermill": {
     "duration": 0.014806,
     "end_time": "2024-05-02T10:56:36.058593",
     "exception": false,
     "start_time": "2024-05-02T10:56:36.043787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 59, 52, 70, 58, 66, 70]\n",
      "chatgpt\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"chatgpt\"))\n",
    "print(decode(encode(\"chatgpt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c30b09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:36.072226Z",
     "iopub.status.busy": "2024-05-02T10:56:36.071964Z",
     "iopub.status.idle": "2024-05-02T10:56:39.991628Z",
     "shell.execute_reply": "2024-05-02T10:56:39.990630Z"
    },
    "id": "iALqoAtd9xA4",
    "papermill": {
     "duration": 3.928777,
     "end_time": "2024-05-02T10:56:39.993720",
     "exception": false,
     "start_time": "2024-05-02T10:56:36.064943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77454]) torch.int64\n",
      "tensor([35,  1,  0,  0, 29, 56, 68, 70, 52,  1, 72, 56, 75,  9,  1, 67, 71, 52,\n",
      "        64, 55, 65,  1, 70, 60, 64, 59, 52,  1, 69, 56, 60, 69,  1, 52, 64, 65,\n",
      "        69,  9,  1, 72, 60,  1, 64, 71, 63,  1, 62, 60, 72, 68, 65,  1, 69, 65,\n",
      "        53, 68, 56,  1, 52,  1, 32, 62, 65, 68, 56, 69, 70, 52,  1, 47, 60, 68,\n",
      "        58, 56, 63,  9,  1,  3, 34, 60, 69, 70, 65, 68, 60, 52, 69,  1,  0, 47,\n",
      "        60, 72, 60, 55, 52, 69,  3,  9,  1, 71, 63, 52,  1, 60, 63, 66, 65, 64,\n",
      "        56, 64, 70, 56,  1, 58, 68, 52, 72, 71, 68, 52, 11,  1, 43, 56, 66, 68,\n",
      "        56, 69, 56, 64, 70, 52, 72, 52,  1, 56, 62, 52,  1, 71, 63, 52,  1, 61,\n",
      "        60, 53, 65, 60, 52,  1, 67, 71, 56,  1, 56, 64, 58, 65, 62, 60, 52,  1,\n",
      "        71, 63, 52,  1, 57, 56, 68, 52, 11,  1, 31, 60, 69,  1,  0, 52,  1, 54,\n",
      "        65, 66, 60, 52,  1, 55, 65,  1, 55, 56, 69, 56, 64, 59, 65, 11,  1,  0,\n",
      "         0,  0,  0, 30, 60, 75, 60, 52,  1, 65,  1, 62, 60, 72, 68, 65, 22,  1,\n",
      "         3, 27, 69,  1, 61, 60, 53, 65, 60, 52, 69,  1, 56, 64, 58, 65, 62, 56,\n",
      "        63,  9,  1, 69, 56, 63,  1, 63, 52, 69, 70, 60, 58, 52, 68,  9,  1, 52,\n",
      "         1, 66, 68, 56, 69, 52,  1, 60, 64, 70, 56, 60, 68, 52, 11,  1, 31, 63,\n",
      "         1, 69, 56, 58, 71, 60, 55, 52,  9,  1, 64, 52, 65,  1,  0, 66, 65, 55,\n",
      "        56, 63,  1, 63, 65, 72, 56, 68, 10, 69, 56,  1, 56,  1, 55, 65, 68, 63,\n",
      "        56, 63,  1, 65, 69,  1, 69, 56, 60, 69,  1, 63, 56, 69, 56, 69,  1, 55,\n",
      "        52,  1, 55, 60, 58, 56, 69, 70, 52, 65, 11,  3,  1,  0,  0, 43, 56, 57,\n",
      "        62, 56, 70, 60,  1, 63, 71, 60, 70, 65,  1, 56, 64, 70, 52, 65,  1, 69,\n",
      "        65, 53, 68, 56,  1, 52, 69,  1, 52, 72, 56, 64, 70, 71, 68, 52, 69,  1,\n",
      "        55, 52,  1, 69, 56, 62, 72, 52,  9,  1, 56,  1, 57, 60, 75,  9,  1, 54,\n",
      "        65, 63,  1, 62, 52, 66, 60, 69,  1, 55, 56,  1, 54, 65, 68,  9,  1, 65,\n",
      "         1, 63, 56, 71,  1,  0, 66, 68, 60, 63, 56, 60, 68, 65,  1, 55, 56, 69,\n",
      "        56, 64, 59, 65, 11,  1, 38, 56, 71,  1, 55, 56, 69, 56, 64, 59, 65,  1,\n",
      "        64, 71, 63, 56, 68, 65,  1, 14,  1, 56, 68, 52,  1, 52, 69, 69, 60, 63,\n",
      "        22,  1,  0,  0,  0,  0, 38, 65, 69, 70, 68, 56, 60,  1, 63, 60, 64, 59,\n",
      "        52,  1, 65, 53, 68, 52, 10, 66, 68, 60, 63, 52,  1, 52, 69,  1, 66, 56,\n",
      "        69, 69, 65, 52, 69,  1, 58, 68, 52, 64, 55, 56, 69,  1, 56,  1, 66, 56,\n",
      "        68, 58, 71, 64, 70, 56, 60,  1, 69, 56,  1, 65,  1, 63, 56, 71,  1, 55,\n",
      "        56, 69, 56, 64, 59, 65,  1, 62, 59, 56, 69,  1,  0, 57, 52, 75, 60, 52,\n",
      "         1, 63, 56, 55, 65, 11,  1,  0,  0, 43, 56, 69, 66, 65, 64, 55, 56, 68,\n",
      "        52, 63, 10, 63, 56, 22,  1,  3, 41, 65, 68,  1, 67, 71, 56,  1, 56,  1,\n",
      "        67, 71, 56,  1, 71, 63,  1, 54, 59, 52, 66, 56, 71,  1, 57, 52, 68, 60,\n",
      "        52,  1, 63, 56, 55, 65, 26,  3,  1,  0,  0, 38, 56, 71,  1, 55, 56, 69,\n",
      "        56, 64, 59, 65,  1, 64, 52, 65,  1, 68, 56, 66, 68, 56, 69, 56, 64, 70,\n",
      "        52, 72, 52,  1, 71, 63,  1, 54, 59, 52, 66, 56, 71, 11,  1, 43, 56, 66,\n",
      "        68, 56, 69, 56, 64, 70, 52, 72, 52,  1, 71, 63, 52,  1, 61, 60, 53, 65,\n",
      "        60, 52,  1, 55, 60, 58, 56, 68, 60, 64, 55, 65,  1, 71, 63,  1,  0, 56,\n",
      "        62, 56, 57, 52, 64, 70, 56, 11,  1, 30, 56, 69, 56, 64, 59, 56, 60,  1,\n",
      "        56, 64, 70, 52, 65,  1, 65,  1, 60, 64, 70, 56, 68, 60, 65, 68,  1, 55,\n",
      "        52,  1, 61, 60, 53, 65, 60, 52,  9,  1, 52,  1, 57, 60, 63,  1, 55, 56,\n",
      "         1, 67, 71, 56,  1, 52, 69,  1, 66, 56, 69, 69, 65, 52, 69,  1, 58, 68,\n",
      "        52, 64, 55, 56, 69,  1, 66, 71, 55, 56, 69, 69, 56, 63,  1,  0, 54, 65,\n",
      "        63, 66, 68, 56, 56, 64, 55, 56, 68, 11,  1, 31, 62, 52, 69,  1, 70, 56,\n",
      "        68, 64,  1, 69, 56, 63, 66, 68, 56,  1, 64, 56, 54, 56, 69, 69, 60, 55,\n",
      "        52, 55, 56,  1, 55, 56,  1, 56, 73, 66, 62, 60, 54, 52, 70, 56, 69,  1,\n",
      "        11,  1, 38, 56, 71,  1, 55, 56, 69, 56, 64, 59, 65,  1, 64, 71, 63, 56,\n",
      "        68, 65,  1, 15,  1, 56, 68, 52,  1,  0, 52, 69, 69, 60, 63, 22,  1,  0,\n",
      "         0,  0,  0, 27, 69,  1, 66, 56, 69, 69, 65, 52, 69,  1, 58, 68, 52, 64,\n",
      "        55, 56, 69,  1, 52, 54, 65, 64, 69, 56, 62, 59, 52, 68, 52, 63, 10, 63,\n",
      "        56,  1, 55, 56, 60, 73, 52, 68,  1, 55, 56,  1, 62, 52, 55, 65,  1, 65,\n",
      "        69,  1, 55, 56, 69, 56, 64, 59, 65, 69,  1, 55, 56,  1, 61, 60, 53, 65,\n",
      "        60, 52, 69,  1, 52, 53, 56, 68, 70, 52, 69,  1,  0, 65, 71,  1, 57, 56,\n",
      "        54, 59, 52, 55, 52, 69,  9,  1, 56,  1, 55, 56, 55, 60, 54, 52, 68, 10,\n",
      "        63, 56,  1, 55, 56,  1, 66, 68, 56, 57])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8697ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.008081Z",
     "iopub.status.busy": "2024-05-02T10:56:40.007704Z",
     "iopub.status.idle": "2024-05-02T10:56:40.012026Z",
     "shell.execute_reply": "2024-05-02T10:56:40.011206Z"
    },
    "id": "dEWgUL2BMeVS",
    "papermill": {
     "duration": 0.01342,
     "end_time": "2024-05-02T10:56:40.013769",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.000349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a4382d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.027726Z",
     "iopub.status.busy": "2024-05-02T10:56:40.027466Z",
     "iopub.status.idle": "2024-05-02T10:56:40.034132Z",
     "shell.execute_reply": "2024-05-02T10:56:40.033395Z"
    },
    "papermill": {
     "duration": 0.015684,
     "end_time": "2024-05-02T10:56:40.035898",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.020214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35,  1,  0,  0, 29, 56, 68, 70, 52])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90001444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.050021Z",
     "iopub.status.busy": "2024-05-02T10:56:40.049739Z",
     "iopub.status.idle": "2024-05-02T10:56:40.056524Z",
     "shell.execute_reply": "2024-05-02T10:56:40.055638Z"
    },
    "papermill": {
     "duration": 0.015869,
     "end_time": "2024-05-02T10:56:40.058398",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.042529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([35]) the target: 1\n",
      "when input is tensor([35,  1]) the target: 0\n",
      "when input is tensor([35,  1,  0]) the target: 0\n",
      "when input is tensor([35,  1,  0,  0]) the target: 29\n",
      "when input is tensor([35,  1,  0,  0, 29]) the target: 56\n",
      "when input is tensor([35,  1,  0,  0, 29, 56]) the target: 68\n",
      "when input is tensor([35,  1,  0,  0, 29, 56, 68]) the target: 70\n",
      "when input is tensor([35,  1,  0,  0, 29, 56, 68, 70]) the target: 52\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3679dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.073557Z",
     "iopub.status.busy": "2024-05-02T10:56:40.073308Z",
     "iopub.status.idle": "2024-05-02T10:56:40.109116Z",
     "shell.execute_reply": "2024-05-02T10:56:40.108165Z"
    },
    "papermill": {
     "duration": 0.045639,
     "end_time": "2024-05-02T10:56:40.111512",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.065873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[52, 68, 65,  9,  1, 57, 52, 75],\n",
      "        [ 1, 66, 65, 68,  1, 63, 60, 64],\n",
      "        [54, 52, 69, 52,  2,  1,  0,  0],\n",
      "        [68,  1, 55, 60, 52,  9,  1, 65]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[68, 65,  9,  1, 57, 52, 75, 56],\n",
      "        [66, 65, 68,  1, 63, 60, 64, 59],\n",
      "        [52, 69, 52,  2,  1,  0,  0,  0],\n",
      "        [ 1, 55, 60, 52,  9,  1, 65, 69]])\n",
      "----\n",
      "when input is [52] the target: 68\n",
      "when input is [52, 68] the target: 65\n",
      "when input is [52, 68, 65] the target: 9\n",
      "when input is [52, 68, 65, 9] the target: 1\n",
      "when input is [52, 68, 65, 9, 1] the target: 57\n",
      "when input is [52, 68, 65, 9, 1, 57] the target: 52\n",
      "when input is [52, 68, 65, 9, 1, 57, 52] the target: 75\n",
      "when input is [52, 68, 65, 9, 1, 57, 52, 75] the target: 56\n",
      "when input is [1] the target: 66\n",
      "when input is [1, 66] the target: 65\n",
      "when input is [1, 66, 65] the target: 68\n",
      "when input is [1, 66, 65, 68] the target: 1\n",
      "when input is [1, 66, 65, 68, 1] the target: 63\n",
      "when input is [1, 66, 65, 68, 1, 63] the target: 60\n",
      "when input is [1, 66, 65, 68, 1, 63, 60] the target: 64\n",
      "when input is [1, 66, 65, 68, 1, 63, 60, 64] the target: 59\n",
      "when input is [54] the target: 52\n",
      "when input is [54, 52] the target: 69\n",
      "when input is [54, 52, 69] the target: 52\n",
      "when input is [54, 52, 69, 52] the target: 2\n",
      "when input is [54, 52, 69, 52, 2] the target: 1\n",
      "when input is [54, 52, 69, 52, 2, 1] the target: 0\n",
      "when input is [54, 52, 69, 52, 2, 1, 0] the target: 0\n",
      "when input is [54, 52, 69, 52, 2, 1, 0, 0] the target: 0\n",
      "when input is [68] the target: 1\n",
      "when input is [68, 1] the target: 55\n",
      "when input is [68, 1, 55] the target: 60\n",
      "when input is [68, 1, 55, 60] the target: 52\n",
      "when input is [68, 1, 55, 60, 52] the target: 9\n",
      "when input is [68, 1, 55, 60, 52, 9] the target: 1\n",
      "when input is [68, 1, 55, 60, 52, 9, 1] the target: 65\n",
      "when input is [68, 1, 55, 60, 52, 9, 1, 65] the target: 69\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183d24e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.129226Z",
     "iopub.status.busy": "2024-05-02T10:56:40.128602Z",
     "iopub.status.idle": "2024-05-02T10:56:40.134538Z",
     "shell.execute_reply": "2024-05-02T10:56:40.133409Z"
    },
    "papermill": {
     "duration": 0.016475,
     "end_time": "2024-05-02T10:56:40.136527",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.120052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52, 68, 65,  9,  1, 57, 52, 75],\n",
      "        [ 1, 66, 65, 68,  1, 63, 60, 64],\n",
      "        [54, 52, 69, 52,  2,  1,  0,  0],\n",
      "        [68,  1, 55, 60, 52,  9,  1, 65]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d36a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.152228Z",
     "iopub.status.busy": "2024-05-02T10:56:40.151471Z",
     "iopub.status.idle": "2024-05-02T10:56:40.278964Z",
     "shell.execute_reply": "2024-05-02T10:56:40.278078Z"
    },
    "papermill": {
     "duration": 0.137291,
     "end_time": "2024-05-02T10:56:40.280858",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.143567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 80])\n",
      "tensor(4.7658, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "TRa”b?-5V09;RGa2DOOM;s!\"aBPf-5Ajxsh-7xO/X;!CL1*!C)EfPe>R608IO3Gcd¥DRLPGaChL*vd(G*aLrFJi!pqZ.9C=!—7z2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a57b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:40.296027Z",
     "iopub.status.busy": "2024-05-02T10:56:40.295787Z",
     "iopub.status.idle": "2024-05-02T10:56:42.930728Z",
     "shell.execute_reply": "2024-05-02T10:56:42.929905Z"
    },
    "papermill": {
     "duration": 2.645088,
     "end_time": "2024-05-02T10:56:42.933119",
     "exception": false,
     "start_time": "2024-05-02T10:56:40.288031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5162cad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:42.948677Z",
     "iopub.status.busy": "2024-05-02T10:56:42.948345Z",
     "iopub.status.idle": "2024-05-02T10:56:43.219680Z",
     "shell.execute_reply": "2024-05-02T10:56:43.218420Z"
    },
    "papermill": {
     "duration": 0.281477,
     "end_time": "2024-05-02T10:56:43.221908",
     "exception": false,
     "start_time": "2024-05-02T10:56:42.940431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.583566188812256\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d5d5a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T10:56:43.238559Z",
     "iopub.status.busy": "2024-05-02T10:56:43.238265Z",
     "iopub.status.idle": "2024-05-02T11:00:56.860197Z",
     "shell.execute_reply": "2024-05-02T11:00:56.859201Z"
    },
    "papermill": {
     "duration": 253.632864,
     "end_time": "2024-05-02T11:00:56.862392",
     "exception": false,
     "start_time": "2024-05-02T10:56:43.229528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211664 M parameters\n",
      "step 0: train loss 4.6211, val loss 4.6277\n",
      "step 100: train loss 2.4508, val loss 2.4307\n",
      "step 200: train loss 2.3232, val loss 2.3055\n",
      "step 300: train loss 2.2472, val loss 2.2137\n",
      "step 400: train loss 2.1608, val loss 2.1433\n",
      "step 500: train loss 2.0860, val loss 2.0706\n",
      "step 600: train loss 2.0224, val loss 2.0220\n",
      "step 700: train loss 1.9692, val loss 1.9860\n",
      "step 800: train loss 1.9365, val loss 1.9539\n",
      "step 900: train loss 1.8848, val loss 1.9147\n",
      "step 1000: train loss 1.8519, val loss 1.8798\n",
      "step 1100: train loss 1.8293, val loss 1.8694\n",
      "step 1200: train loss 1.7961, val loss 1.8524\n",
      "step 1300: train loss 1.7852, val loss 1.8331\n",
      "step 1400: train loss 1.7413, val loss 1.8059\n",
      "step 1500: train loss 1.7197, val loss 1.7891\n",
      "step 1600: train loss 1.6927, val loss 1.7638\n",
      "step 1700: train loss 1.6675, val loss 1.7572\n",
      "step 1800: train loss 1.6313, val loss 1.7215\n",
      "step 1900: train loss 1.6041, val loss 1.7239\n",
      "step 2000: train loss 1.5837, val loss 1.7010\n",
      "step 2100: train loss 1.5925, val loss 1.7111\n",
      "step 2200: train loss 1.5611, val loss 1.6917\n",
      "step 2300: train loss 1.5494, val loss 1.6994\n",
      "step 2400: train loss 1.5313, val loss 1.6750\n",
      "step 2500: train loss 1.5253, val loss 1.6976\n",
      "step 2600: train loss 1.5115, val loss 1.6709\n",
      "step 2700: train loss 1.4920, val loss 1.6610\n",
      "step 2800: train loss 1.4905, val loss 1.6769\n",
      "step 2900: train loss 1.4683, val loss 1.6512\n",
      "step 3000: train loss 1.4583, val loss 1.6296\n",
      "step 3100: train loss 1.4486, val loss 1.6504\n",
      "step 3200: train loss 1.4163, val loss 1.6383\n",
      "step 3300: train loss 1.4191, val loss 1.6366\n",
      "step 3400: train loss 1.3983, val loss 1.6367\n",
      "step 3500: train loss 1.4016, val loss 1.6223\n",
      "step 3600: train loss 1.3711, val loss 1.6150\n",
      "step 3700: train loss 1.3825, val loss 1.6389\n",
      "step 3800: train loss 1.3644, val loss 1.6449\n",
      "step 3900: train loss 1.3499, val loss 1.6451\n",
      "step 4000: train loss 1.3446, val loss 1.6363\n",
      "step 4100: train loss 1.3412, val loss 1.6337\n",
      "step 4200: train loss 1.3295, val loss 1.6164\n",
      "step 4300: train loss 1.3123, val loss 1.6205\n",
      "step 4400: train loss 1.3087, val loss 1.6195\n",
      "step 4500: train loss 1.2956, val loss 1.6236\n",
      "step 4600: train loss 1.2926, val loss 1.6353\n",
      "step 4700: train loss 1.2832, val loss 1.6332\n",
      "step 4800: train loss 1.2708, val loss 1.6435\n",
      "step 4900: train loss 1.2673, val loss 1.6268\n",
      "step 4999: train loss 1.2597, val loss 1.6357\n",
      "\n",
      "\n",
      "\n",
      "Sabiamente uma por um pouco \n",
      "do pogos, no tambeirao, que ele, \n",
      "descer, compridas estrele a percebes e mesmos sei, ao mnarao \n",
      "absariao Sois da rei perguntadoso, a jiboia, sao lovas tigoistes sao se sabia, umao facal em lampiao. \n",
      "\n",
      "- Eu vermer uma validadei mim: \n",
      "\n",
      "- E as umao a Tinha vida, ou diago \n",
      "dobriloes do setentos vou \n",
      "possenul12\". Um ate cabego entao, tento nao encomplendeu. \n",
      "\n",
      "Mas o garo que eu mesmo incipezinho. \n",
      "\n",
      "Mas recomos quas se ela rei. \n",
      "\n",
      "- Simpitas tao ouseriam ao principe. Siintas de tao bem. Em exasmento demai um porquilo dera. Saudou a relveu, . \" Mas o \n",
      "laga de ou lampioes sao efim. Signinte de estur triste deiroso, que conso \n",
      "as quiles acabavam os principe. \n",
      "\n",
      "Viu-ses planeta e tao colameiro sertias, a disto . \" Desia vozinha um ameiro. \n",
      "\n",
      "A flor que ele, guarda-me dessava-lo ou \n",
      "\n",
      "principezinho, e respondeu logo delas. \n",
      "\n",
      "\n",
      "E ao palansamente aladou, apareira ao sil. \n",
      "\n",
      "As planeta eu recepo a visgulhava consigo, por confunuie em. Depois, quis dizer: \n",
      "\n",
      "- Sao hadmiradar ao sete, ao sei muito cadava de antor. Eu a moraida... Faziaga ... \n",
      "\n",
      "\n",
      "- O” que fala! \n",
      "\n",
      "Ela vessistou, sao o setei, pos ontigos de interir, \n",
      "param de amer relico do \n",
      "a9ares prontao, entao saia doia acabansarravaliau-me \n",
      "no terlendo julgao. \n",
      "Conseri universo. Esfoi-lhem, tambem a galinhas. \n",
      "\n",
      "Mas eu abedei as estrelas e quatro horavas de decada. . \" Foi mais el que perdagao. \n",
      "\n",
      "- Bom ! explorades. E, pois no maio. \n",
      "\n",
      "Possa seu mes amigo. Eu o principezinho numeromo numero, nao flor destapete. \n",
      "Desponsavam: \n",
      "\n",
      "- Sobe que, respondeu o rei. Sera \n",
      "ou o pogo, o meu leve mecuto. \n",
      "\n",
      "Mas ele, que as fazes irmigoso o tercei-lo. Por tambem ... \n",
      "\n",
      "\n",
      "\n",
      "- Sem duvida explor uma morrota909as. A galinhas ritem debias que ais da voltantes, onda minha perdidamente o um meu dobedo, e todos no oitos. \n",
      "\n",
      "\n",
      "/ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nao o principezinho possuiu-se, e em viu encomosto. \n",
      "\n",
      "Entao enconto pois ano ao nego muito a minha milhoes ! Da \n",
      "ti ao agua: \n",
      "\n",
      "O principezinho, perfumuialamente dos adminos. Tu ago, da me, bem a \n",
      "unive e \n",
      "nos \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('/kaggle/input/pequeno-p-text/pequenop.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2a0ff",
   "metadata": {
    "papermill": {
     "duration": 0.01122,
     "end_time": "2024-05-02T11:00:56.885234",
     "exception": false,
     "start_time": "2024-05-02T11:00:56.874014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Acknowledgements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c029764",
   "metadata": {
    "papermill": {
     "duration": 0.010972,
     "end_time": "2024-05-02T11:00:56.907379",
     "exception": false,
     "start_time": "2024-05-02T11:00:56.896407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Google colab for the video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=hoelkOrFY8bN)\n",
    "\n",
    "[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1737s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a9eb7",
   "metadata": {
    "papermill": {
     "duration": 0.011326,
     "end_time": "2024-05-02T11:00:56.929958",
     "exception": false,
     "start_time": "2024-05-02T11:00:56.918632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In regards to the original code, **there's only a minor change in the dataset used, with the rest copied and studied as a means to grasp the concepts** (within my limitations)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4921620,
     "sourceId": 8286341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4921648,
     "sourceId": 8286377,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 265.469355,
   "end_time": "2024-05-02T11:00:58.362516",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-02T10:56:32.893161",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
