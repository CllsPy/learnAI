{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ISDru8PDElUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bj1YDbZNFOhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/season_02/ep01/twitter_validation.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "19KGMCcQFQVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename cols\n",
        "# rename targets\n",
        "\n",
        "df = df.rename(columns={'Irrelevant':'Target', 'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£':'Text'})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vOdtSAoYF55E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column = ['Target', 'Text']\n",
        "df = df[column]\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "_TE6iYHbGg7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map_ = {'Neutral': 0, 'Positive': 1, 'Negative': 2}\n",
        "# df.Target = df.Target.map(map_)\n",
        "# df.head(3)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df.Target = le.fit_transform(df.Target)\n",
        "df\n"
      ],
      "metadata": {
        "id": "Z5qCPJ0aGmX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "id": "sdZrZgd-Yqxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Target = df.Target.fillna(0)\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "hfyg4o3sTHVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Pfs9LLQkG3q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag Of Words"
      ],
      "metadata": {
        "id": "7fP42VpxNAy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)\n",
        "print(count.vocabulary_)"
      ],
      "metadata": {
        "id": "jB8P9LCWNCt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True,\n",
        "                         norm='l2',\n",
        "                         smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs))\n",
        "      .toarray())"
      ],
      "metadata": {
        "id": "lEAprFmlOZwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning\n"
      ],
      "metadata": {
        "id": "vaIMviuqPobW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0, 'Text'][-50:]"
      ],
      "metadata": {
        "id": "dnImakfOPpic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "metadata": {
        "id": "TefSnEVrPq_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor(df.loc[0, 'Text'][-50:])\n",
        "df['Text'] = df['Text'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "jr_7UbNJPz3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "vZiv4g2jP9Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "\n",
        "tokenizer_porter('runners like running and thus they run')"
      ],
      "metadata": {
        "id": "K2eUT3WcP_GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "YtIjxXM_QYSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
        " if w not in stop]"
      ],
      "metadata": {
        "id": "Z2TZTkr9Qf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.iloc[:, 1].values\n",
        "y = df.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "oVb-pWPoQkoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "\"\"\"\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              ]\n",
        "\"\"\"\n",
        "\n",
        "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [None],\n",
        "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                     'clf__C': [1.0, 10.0]},\n",
        "                    {'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [stop, None],\n",
        "                     'vect__tokenizer': [tokenizer],\n",
        "                     'vect__use_idf':[False],\n",
        "                     'vect__norm':[None],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                  'clf__C': [1.0, 10.0]},\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)"
      ],
      "metadata": {
        "id": "9ZJZx6KjRaRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xW7W5b-ES5zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
        "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
      ],
      "metadata": {
        "id": "E_5izwiuXsIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
      ],
      "metadata": {
        "id": "GFUR-xBfXs4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(['love it'])"
      ],
      "metadata": {
        "id": "7JzuPDJiYakR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}