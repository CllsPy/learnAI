# Notes

## GD
Gradient Descent é um algorítimo para encontrar o valor máixmo ou mínimo de uma função, em ML ou DP usamos para minimizar a função de custo.

Gradient Descent não pode ser aplicado em qualquer função, ela precisa ter duas características:
1. diferenciável
2. convexa

**Loss Function**

Uma função de custo é um algorítimo para avaliar o quão bem nosso modelo performou no dataset ou seja quão bem ele aprendeu o padrão.

## Mini Batch SGD

[Mini Batch SGD](https://www.youtube.com/watch?v=FpDsDn-fBKA) é usado quando temos uma grande quantidade de dados.

Mini Batch porque ao invés da atualização dos pesos (w) ocorrer após a análise sobre todas as amostras ela ocorre a cada número de Batch (pedaços).

## Useful resources
- [Tutorial 12- Stochastic Gradient Descent vs Gradient Descent](https://www.youtube.com/watch?v=FpDsDn-fBKA)
- [Gradient descent Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)
- [Understanding Concave and Convex Functions](https://www.youtube.com/watch?v=nOFXLCCvtm0)

## Blog posts mentioned
- [Gradient Descent Algorithm — a deep dive](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)
- [Introduction to Loss Functions](https://socratic.org/calculus/derivatives/differentiable-vs-non-differentiable-functions#:~:text=So%20a%20point%20where%20the,%7Cx%7C%20at%200)
