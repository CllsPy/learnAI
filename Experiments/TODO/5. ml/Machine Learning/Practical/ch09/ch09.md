# Linear Regression 
We can divide in simple and multi-variable problems, the simple one is defioned as follow: y = wix + b, The point is mesure the relationshop between two variables, late you'll see how to expand it to many.

The b here, represents the intercept in the y axis, and w the weights, our mission is to find w that enable us to predict unseen values based on its relationshiop.

The objetive of linear regression is to fit the best possible line, based on our data. 

## Multiple Linear Regression
For a problem with 'n' features, we apply a sum at our original (univariete) equation.
![image](https://github.com/user-attachments/assets/57222005-500a-4f54-92c2-6cb6ef60c71d)


## RANSAC (RANdom SAmple Consensus)
[RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) works by examining if a value has impact (so it means that its a outliers).

- It fit a model in several samples of data and choose the best model (sample with more inliers will perform better)

![image](https://github.com/user-attachments/assets/9d973ec9-6e95-4413-839f-9e34b2c87f77)

dataset with many outliers

![image](https://github.com/user-attachments/assets/213eec56-be96-452c-b76a-c2ef57b11b1d)

RANSAC in the data, outliers has no effect.


**RANSAC**

RANSAC is an acronimous for RANdom SAmple Consensus, we can use to peform linear regression task when the data have lots of outliers. It works as follow:

- fit a model in a sample of the data
- choose the model model (precisely the sample with less outliers)

To decide if a data point is a outliers or not, the model use the MAD concept, which is how far the datapoint is from the mean, we can adjust out mad. To calculate mad, we do: mean(abs(data-mean(data))

Evan RANSAC is really good with outliers by selecting inliers, there is no guaratee that the model i'll have a good performance on unseen data.

## Evaluating Regression Models
Train a model is half of the problem, as a ml engineer you need assure that your model works on unseen data.

## Using regularized methods for regression
Regularization is a weapon agaist overfitting, it adds noise to the data for punish model complexity. For regression some models are:

- ridge
- lasso
- elastic net


Readin List 1: https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning

- Bias implies limiteded flaxibility to learn from data.
- Variance imples high sensitivity to learn from data.

![image](https://github.com/user-attachments/assets/13c50364-c619-46c3-8811-4c0a2dd37375)

- regulaziation
  - We use Regularization when we need reduce complexity, in our case **model complexity.**

- loss
  - The **loss function** is the mesure we use to update our wights and bias.
  - The loss function is used to measure the error in one pair (x, y) at time. The cost function on the other hand is used for calculate the overall error (based on all data points).

- Why increase weight by it's square reduce model complexity?
- What is model complexity?
  - How well your model peform on test data 

- l2 regularization

  ![image](https://github.com/user-attachments/assets/1a0e599b-061a-4b61-8d49-f9aeeff2e4ba)

- what's regularization
  - Is aditional information to adjust the loss.
  
  Links
  - https://developers.google.com/machine-learning/crash-course/overfitting/regularization
  - https://developers.google.com/machine-learning/crash-course/overfitting/model-complexity
  - https://en.wikipedia.org/wiki/Occam's_razor
  - https://developers.google.com/machine-learning/glossary#l2-regularization
