{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## requirements.txt"
      ],
      "metadata": {
        "id": "qh4mXCtHRZtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install youtube-transcript-api\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "QCE1xFITSjtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sumarizer.py"
      ],
      "metadata": {
        "id": "trrGjuwFRcol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def Summarizer(link=\"Video\", model=\"Pegasus\", text=\"Text\"):\n",
        "  if model == \"Pegasus\":\n",
        "      checkpoint = \"google/pegasus-large\"\n",
        "  elif model == \"mT5\":\n",
        "      checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "  elif model == \"BART\":\n",
        "      checkpoint = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "  if link == \"Video\":\n",
        "    video_id = link.split(\"=\")[1]\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"pt\", \"en\"])\n",
        "    FinalTranscript = ' '.join([i['text'] for i in transcript])\n",
        "\n",
        "  else:\n",
        "    FinalTranscript = text\n",
        "\n",
        "  inputs = tokenizer(FinalTranscript,\n",
        "                    max_length=1024,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\")\n",
        "\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"])\n",
        "  summary = tokenizer.batch_decode(summary_ids,\n",
        "                                  skip_special_tokens=True,\n",
        "                                  clean_up_tokenization_spaces=False)\n",
        "\n",
        "\n",
        "  return summary[0]\n"
      ],
      "metadata": {
        "id": "kWgEXZyrRbC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## App.py"
      ],
      "metadata": {
        "id": "unIrkb1CTAp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = gr.Interface(\n",
        "    Summarizer,\n",
        "     [\n",
        "         gr.Textbox(), gr.Textbox(), gr.Textbox()\n",
        "     ],\n",
        "    outputs=\"text\"\n",
        "    )\n",
        "\n",
        "\n",
        "#interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "dRd8YTAETCHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}