{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem: Can't Load Model (Memory Issues)\n\n* S1: Smaller Model\n* S2: https://medium.com/@giacomopedo99/running-transformer-models-without-a-gpu-overcoming-the-flash-attn-dependency-08f785675641\n* S3: Simple use more memory\n* Quantization\n* Quantizing Models: https://www.youtube.com/watch?v=xKAXo0Zkfss","metadata":{"id":"-bawu2x8QY3B"}},{"cell_type":"markdown","source":"**Huh**: Why I need tokenize my prompt?\n\n**Aha**: Maybe GPT, Gemini or when I use Pipeline this thing is happining in the Background;\n\n**Aha**: 8bits is like a medium performance (fast and reasoble answer; 4bits is super fast bit less precise).\n\n**Huh**: Why I'm getting so much text and not the actual answer?\n\n**Q**: How can I load a Large Model using HuggingFace?\n\n**Aha**: Quantization\n**Huh**: Why it gets confuse when we try answer in pt-br?","metadata":{}},{"cell_type":"markdown","source":"## Start","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install accelerate\n!pip install -U bitsandbytes","metadata":{"id":"1M6gETj_NSS_","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:40:53.092271Z","iopub.execute_input":"2025-01-30T16:40:53.092546Z","iopub.status.idle":"2025-01-30T16:41:03.035438Z","shell.execute_reply.started":"2025-01-30T16:40:53.092516Z","shell.execute_reply":"2025-01-30T16:41:03.034228Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## 4Bits-8Bits","metadata":{"id":"vWldQQ2BwpdG"}},{"cell_type":"code","source":"%%capture\nimport torch\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n\nconfig = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    quantization_config=config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"E_PKTOoFwsZ6","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:41:48.207894Z","iopub.execute_input":"2025-01-30T16:41:48.208177Z","iopub.status.idle":"2025-01-30T16:49:06.725710Z","shell.execute_reply.started":"2025-01-30T16:41:48.208154Z","shell.execute_reply":"2025-01-30T16:49:06.725018Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"prompt = \"What's a LLM?\"\n\n# CoT\nmessages = [\n    {\"role\": \"system\", \"content\": \"think step by step\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\n# TIR\n# messages = [\n#     {\"role\": \"system\", \"content\": \"Sua resposta deve ser em pt-br\"},\n#     {\"role\": \"user\", \"content\": prompt}\n# ]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:21:34.077573Z","iopub.execute_input":"2025-01-30T17:21:34.077885Z","iopub.status.idle":"2025-01-30T17:21:45.270519Z","shell.execute_reply.started":"2025-01-30T17:21:34.077859Z","shell.execute_reply":"2025-01-30T17:21:45.269886Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nresponse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:21:49.748555Z","iopub.execute_input":"2025-01-30T17:21:49.748880Z","iopub.status.idle":"2025-01-30T17:21:49.754572Z","shell.execute_reply.started":"2025-01-30T17:21:49.748846Z","shell.execute_reply":"2025-01-30T17:21:49.753805Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'A LLM is a large language model created by Alibaba Cloud. It is a type of natural language processing model that can generate human-like text based on the input it receives. LLMs are used in a variety of applications, including language translation, text generation, and question answering. They are also used in games and simulations to generate realistic scenarios and outcomes.'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"'A LLM is a large language model created by Alibaba Cloud. It is a type of natural language processing model that can generate human-like text based on the input it receives. LLMs are used in a variety of applications, including language translation, text generation, and question answering. They are also used in games and simulations to generate realistic scenarios and outcomes.'\n","metadata":{}}]}